# -*- coding: utf-8 -*-
"""Final_SRC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19dWDABw5aZRD2ViaZMbZDB7vT3MQ-XdK
"""

import sys
print(sys.version)

from google.colab import drive
drive.mount('/content/drive')

import os

folders = [
    "data",
    "src",
    "database"
]

for folder in folders:
    os.makedirs(folder, exist_ok=True)

print("Project folders created:")
print(os.listdir("."))

"""# **Uploading Dataset:**"""

from google.colab import files

uploaded = files.upload()

import shutil

for file in uploaded.keys():
    shutil.move(file, f"data/{file}")

print("Files in data folder:", os.listdir("data"))

"""# **Cross Checking the dataset:**



"""

import csv

def count_rows(path):
    with open(path, newline="", encoding="utf-8") as f:
        return sum(1 for _ in f) - 1

print("Math rows:", count_rows("data/student-mat.csv"))
print("Portuguese rows:", count_rows("data/student-por.csv"))
print("Total:", count_rows("data/student-mat.csv") + count_rows("data/student-por.csv"))

#Delete it after a while

"""# **Parsing the dataset:**

"""

import csv

def parse_csv(file_path, subject_name):
    records = []

    with open(file_path, newline="", encoding="utf-8") as csvfile:
        reader = csv.DictReader(csvfile)

        for row in reader:
            row["subject"] = subject_name
            records.append(row)

    return records


def load_all_records():
    math_records = parse_csv("data/student-mat.csv", "math")
    por_records = parse_csv("data/student-por.csv", "portuguese")

    all_records = math_records + por_records
    return all_records

data = load_all_records()

print("Total records:", len(data))
print("First record:")
print(data[0])

"""# **Creating DB and Tables:**"""

import sqlite3
import os

db_path = "database/students.db"

# Remove existing DB if rerunning
if os.path.exists(db_path):
    os.remove(db_path)

conn = sqlite3.connect(db_path)
cursor = conn.cursor()

# Create tables
cursor.execute("""
CREATE TABLE students (
    student_id INTEGER PRIMARY KEY,
    school TEXT,
    sex TEXT,
    age INTEGER,
    address TEXT
)
""")

cursor.execute("""
CREATE TABLE family_background (
    student_id INTEGER,
    famsize TEXT,
    Pstatus TEXT,
    Medu INTEGER,
    Fedu INTEGER,
    internet TEXT,
    FOREIGN KEY(student_id) REFERENCES students(student_id)
)
""")

cursor.execute("""
CREATE TABLE academics (
    student_id INTEGER,
    studytime INTEGER,
    failures INTEGER,
    absences INTEGER,
    G3 INTEGER,
    subject TEXT,
    at_risk INTEGER,
    FOREIGN KEY(student_id) REFERENCES students(student_id)
)
""")

conn.commit()
conn.close()

print("Database and tables created successfully.")

"""# **Loading Data from CSV parser into DB**"""

import sqlite3

# reuse functions from Layer 1
data = load_all_records()

conn = sqlite3.connect("database/students.db")
cursor = conn.cursor()

student_id = 1

for row in data:
    # STUDENTS table
    cursor.execute("""
        INSERT INTO students VALUES (?, ?, ?, ?, ?)
    """, (
        student_id,
        row["school"],
        row["sex"],
        int(row["age"]),
        row["address"]
    ))

    # FAMILY BACKGROUND table
    cursor.execute("""
        INSERT INTO family_background VALUES (?, ?, ?, ?, ?, ?)
    """, (
        student_id,
        row["famsize"],
        row["Pstatus"],
        int(row["Medu"]),
        int(row["Fedu"]),
        row["internet"]
    ))

    # ACADEMICS table
    g3 = int(row["G3"])
    at_risk = 1 if g3 < 10 else 0

    cursor.execute("""
        INSERT INTO academics VALUES (?, ?, ?, ?, ?, ?, ?)
    """, (
        student_id,
        int(row["studytime"]),
        int(row["failures"]),
        int(row["absences"]),
        g3,
        row["subject"],
        at_risk
    ))

    student_id += 1

conn.commit()
conn.close()

print("Data inserted into database successfully.")

conn = sqlite3.connect("database/students.db")
cursor = conn.cursor()

cursor.execute("SELECT COUNT(*) FROM students")
print("Students:", cursor.fetchone()[0])

cursor.execute("SELECT COUNT(*) FROM academics")
print("Academics:", cursor.fetchone()[0])

cursor.execute("SELECT COUNT(*) FROM family_background")
print("Family background:", cursor.fetchone()[0])

conn.close()

"""# **Joining 3 Tables:**"""

import sqlite3
import pandas as pd

conn = sqlite3.connect("database/students.db")

query = """
SELECT
    s.student_id,
    s.school,
    s.sex,
    s.age,
    s.address,

    f.famsize,
    f.Pstatus,
    f.Medu,
    f.Fedu,
    f.internet,

    a.studytime,
    a.failures,
    a.absences,
    a.subject,
    a.at_risk
FROM students s
JOIN family_background f
    ON s.student_id = f.student_id
JOIN academics a
    ON s.student_id = a.student_id
"""

df = pd.read_sql_query(query, conn)
conn.close()

print("DataFrame shape:", df.shape)
df.head()

print(df.isnull().sum())
print(df["at_risk"].value_counts())

# Target variable
y = df["at_risk"]

# Drop non-feature columns
X = df.drop(columns=["student_id", "at_risk"])

categorical_cols = X.select_dtypes(include=["object"]).columns.tolist()
numerical_cols = X.select_dtypes(exclude=["object"]).columns.tolist()

print("Categorical columns:", categorical_cols)
print("Numerical columns:", numerical_cols)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

!pip install ydata-profiling

"""EDA:"""

from ydata_profiling import ProfileReport

profile = ProfileReport(
    df,
    title="Student Risk Classification ‚Äì EDA Report",
    explorative=True
)

profile

profile.to_file("student_eda_report.html")

"""Correlation Matrix"""

df_encoded = pd.get_dummies(df, drop_first=True)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(14,10))
corr = df_encoded.corr()

sns.heatmap(
    corr,
    cmap="coolwarm",
    center=0,
    linewidths=0.5
)

plt.title("Correlation Matrix")
plt.show()

"""### üîç EDA Observations

- The target variable `at_risk` is imbalanced, with fewer students classified as at-risk.
- This justifies the use of stratified train/test split and F1-score as the primary evaluation metric.
- The feature `absences` shows right skewness with extreme values.
- `studytime` is ordinal and capped between 1 and 4.
- `G3` is capped between 0 and 20.
- `failures` shows positive correlation with `at_risk`.
- `studytime` shows negative correlation with `at_risk`.

### üßπ Data Cleanup Tasks Identified

1. Encode categorical variables using OneHotEncoding.
2. Scale numerical features using StandardScaler or MinMaxScaler.
3. Apply log transformation to skewed features like `absences` if needed.
4. Use stratified sampling to handle class imbalance.
5. Use feature selection techniques to remove correlated features.
6. Apply PCA for dimensionality reduction in later experiments.

# **Experiment #1 Imports**
"""

!pip install mlflow dagshub scikit-learn

import dagshub
import mlflow
import os

os.environ["DAGSHUB_USER_TOKEN"] = "96a7fb9832babfcd5ae534545485a4c659c897cc"

dagshub.init(
    repo_owner="Khasim0210",
    repo_name="student-risk-classification",
    mlflow=True
)

print("‚úÖ MLflow connected to DagsHub successfully")

# ==============================
# Experiment #1: Logistic Regression with CV & Hyperparameter Tuning
# ==============================

from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import f1_score, confusion_matrix
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

numeric_standard = Pipeline(steps=[
    ("scaler", StandardScaler())
])

numeric_minmax = Pipeline(steps=[
    ("scaler", MinMaxScaler())
])

categorical_transformer = Pipeline(steps=[
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_standard, numerical_cols),
        ("cat", categorical_transformer, categorical_cols)
    ]
)

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression(
    max_iter=2000,
    solver="liblinear"
)

"""Building the pipline

"""

pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", log_reg)
])

"""Hyperparameter Grid"""

param_grid = {
    "preprocessor__num__scaler": [
        StandardScaler(),
        MinMaxScaler()
    ],
    "classifier__C": [0.01, 0.1, 1, 10],
    "classifier__penalty": ["l1", "l2"]
}

"""Cross-validation -set up"""

cv = StratifiedKFold(
    n_splits=5,
    shuffle=True,
    random_state=42
)

grid_search = GridSearchCV(
    pipeline,
    param_grid=param_grid,
    scoring="f1",
    cv=cv,
    n_jobs=-1,
    return_train_score=True
)

grid_search.fit(X_train, y_train)

"""Extracting the best Result."""

best_model = grid_search.best_estimator_

cv_results = grid_search.cv_results_

mean_f1 = grid_search.best_score_
std_f1 = cv_results["std_test_score"][grid_search.best_index_]

print("Best CV F1 Score:", mean_f1)
print("CV Std Dev:", std_f1)
print("Best Parameters:", grid_search.best_params_)

"""Evalute Test set"""

from sklearn.metrics import confusion_matrix, f1_score

y_pred = best_model.predict(X_test)

test_f1 = f1_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

tn, fp, fn, tp = cm.ravel()

print("Test F1 Score:", test_f1)
print("Confusion Matrix:")
print(cm)
print("TP:", tp, "TN:", tn, "FP:", fp, "FN:", fn)

with mlflow.start_run(run_name="Experiment_1_LogisticRegression"):

    mlflow.log_param("model", "LogisticRegression")
    mlflow.log_params(grid_search.best_params_)
    mlflow.log_param("cv_folds", 5)

    mlflow.log_metric("cv_mean_f1", mean_f1)
    mlflow.log_metric("cv_std_f1", std_f1)
    mlflow.log_metric("test_f1", test_f1)

    mlflow.log_metric("TP", tp)
    mlflow.log_metric("TN", tn)
    mlflow.log_metric("FP", fp)
    mlflow.log_metric("FN", fn)

    mlflow.sklearn.log_model(best_model, "model")

"""### üß™ Experiment #1 Summary

In this experiment, a complete preprocessing and classification pipeline was
constructed using Logistic Regression. Numerical features were scaled using
either StandardScaler or MinMaxScaler, selected via hyperparameter tuning.
GridSearchCV with 5-fold stratified cross-validation was used to optimize
hyperparameters. Due to class imbalance, F1-score was chosen as the primary
evaluation metric. Mean and standard deviation of cross-validation scores,
confusion matrix values, and the final trained model were logged using MLflow
on DagsHub.

Experiment #2
"""

from sklearn.linear_model import RidgeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score

!pip install xgboost

from xgboost import XGBClassifier

models = {
    "LogisticRegression": LogisticRegression(
        max_iter=2000,
        solver="liblinear"
    ),
    "RidgeClassifier": RidgeClassifier(),
    "RandomForest": RandomForestClassifier(
        n_estimators=200,
        random_state=42
    ),
    "XGBoost": XGBClassifier(
        n_estimators=200,
        learning_rate=0.1,
        max_depth=5,
        subsample=0.8,
        colsample_bytree=0.8,
        eval_metric="logloss",
        random_state=42
    )
}

"""Train, Evaluate & Log Each Model"""

for model_name, model in models.items():

    pipeline = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("classifier", model)
    ])

    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)

    f1 = f1_score(y_test, y_pred)

    with mlflow.start_run(run_name=f"Experiment_2_{model_name}"):

        mlflow.log_param("model", model_name)
        mlflow.log_metric("test_f1", f1)

        mlflow.sklearn.log_model(pipeline, "model")

    print(f"{model_name} ‚Äî Test F1 Score: {f1}")

"""Compare Results"""

results = []

for model_name, model in models.items():
    pipeline = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("classifier", model)
    ])
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    f1 = f1_score(y_test, y_pred)
    results.append((model_name, f1))

results

"""### üß™ Experiment #2 Summary

In this experiment, multiple classification models were evaluated using a
consistent preprocessing pipeline. Logistic Regression, Ridge Classifier,
Random Forest, and XGBoost were trained and compared using F1-score on the
test set. All model performances and artifacts were logged to MLflow for
comparison and model selection.

# **Experiment #3**
"""

X_fe = X.copy()

# Study efficiency
X_fe["study_efficiency"] = X_fe["studytime"] / (X_fe["absences"] + 1)

# Parental education average
X_fe["parent_edu_avg"] = (X_fe["Medu"] + X_fe["Fedu"]) / 2

# Failure flag
X_fe["has_failed_before"] = (X_fe["failures"] > 0).astype(int)

# Age group (categorical)
X_fe["age_group"] = pd.cut(
    X_fe["age"],
    bins=[0, 16, 18, 100],
    labels=["<17", "17-18", ">18"]
)

X_fe.head()

categorical_cols_fe = X_fe.select_dtypes(include=["object", "category"]).columns.tolist()
numerical_cols_fe = X_fe.select_dtypes(exclude=["object", "category"]).columns.tolist()

print("Categorical (FE):", categorical_cols_fe)
print("Numerical (FE):", numerical_cols_fe)

X_train_fe, X_test_fe, y_train_fe, y_test_fe = train_test_split(
    X_fe,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

preprocessor_fe = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numerical_cols_fe),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols_fe)
    ]
)

pipeline_fe = Pipeline(steps=[
    ("preprocessor", preprocessor_fe),
    ("classifier", LogisticRegression(
        max_iter=2000,
        solver="liblinear"
    ))
])

pipeline_fe.fit(X_train_fe, y_train_fe)

y_pred_fe = pipeline_fe.predict(X_test_fe)

f1_fe = f1_score(y_test_fe, y_pred_fe)

print("Feature Engineered Model F1:", f1_fe)

with mlflow.start_run(run_name="Experiment_3_Feature_Engineering"):

    mlflow.log_param("engineered_features", [
        "study_efficiency",
        "parent_edu_avg",
        "has_failed_before",
        "age_group"
    ])

    mlflow.log_metric("test_f1", f1_fe)

    mlflow.sklearn.log_model(pipeline_fe, "model")

"""### üß™ Experiment #3 Summary ‚Äì Feature Engineering

In this experiment, new features were engineered based on domain knowledge,
including study efficiency, parental education average, prior failure flag,
and age groups. These engineered features were added to the dataset and a
Logistic Regression model was retrained. The resulting model performance was
evaluated using F1-score and logged using MLflow for comparison with previous
experiments.

## **Experiment #4**
"""

X_fe_encoded = pd.get_dummies(X_fe, drop_first=True)

"""Correlation Threshold"""

corr_matrix = X_fe_encoded.corr().abs()

upper_tri = corr_matrix.where(
    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
)

correlation_threshold = 0.9

to_drop_corr = [
    column for column in upper_tri.columns
    if any(upper_tri[column] > correlation_threshold)
]

print("Features dropped due to high correlation:")
print(to_drop_corr)

"""Variance Threshold"""

from sklearn.feature_selection import VarianceThreshold

var_selector = VarianceThreshold(threshold=0.01)
X_var_selected = var_selector.fit_transform(X_fe_encoded)

selected_var_features = X_fe_encoded.columns[var_selector.get_support()]

print("Features kept after variance thresholding:")
print(len(selected_var_features))

"""Feature Importance (Tree-based)"""

from sklearn.ensemble import RandomForestClassifier

# Split the ENCODED feature-engineered data
X_train_enc, X_test_enc, y_train_enc, y_test_enc = train_test_split(
    X_fe_encoded,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

rf = RandomForestClassifier(
    n_estimators=200,
    random_state=42
)

rf.fit(X_train_enc, y_train_enc)

importances = rf.feature_importances_

feature_importance_df = pd.DataFrame({
    "feature": X_fe_encoded.columns,
    "importance": importances
}).sort_values(by="importance", ascending=False)

feature_importance_df.head(10)

"""Select Top N Important Features"""

top_n = 15
top_features = feature_importance_df.head(top_n)["feature"].tolist()

print("Top selected features:")
print(top_features)

"""Train Model"""

X_train_sel = X_train_enc[top_features]
X_test_sel = X_test_enc[top_features]

pipeline_sel = Pipeline(steps=[
    ("scaler", StandardScaler()),
    ("classifier", LogisticRegression(
        max_iter=2000,
        solver="liblinear"
    ))
])

pipeline_sel.fit(X_train_sel, y_train_sel := y_train_fe)

"""Evaluation"""

y_pred_sel = pipeline_sel.predict(X_test_sel)

f1_sel = f1_score(y_test_fe, y_pred_sel)

print("Feature Selected Model F1:", f1_sel)

"""**MLflow Logging**"""

with mlflow.start_run(run_name="Experiment_4_Feature_Selection"):

    mlflow.log_param("correlation_threshold", correlation_threshold)
    mlflow.log_param("variance_threshold", 0.01)
    mlflow.log_param("top_n_features", top_n)

    mlflow.log_metric("test_f1", f1_sel)

    mlflow.log_param("selected_features", top_features)

    mlflow.sklearn.log_model(pipeline_sel, "model")

"""### üß™ Experiment #4 Summary ‚Äì Feature Selection

In this experiment, multiple feature selection techniques were applied.
Highly correlated features were removed using a correlation threshold,
low-variance features were filtered out using variance thresholding, and
feature importance was computed using a Random Forest classifier. The top
important features were selected and used to retrain a Logistic Regression
model. The resulting performance was evaluated using F1-score and logged
using MLflow.

## **Experiment #5**

Standardize Data
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_fe_encoded)

from sklearn.decomposition import PCA

pca = PCA()
X_pca = pca.fit_transform(X_scaled)

"""Scree Plot"""

import matplotlib.pyplot as plt
import numpy as np

explained_variance = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance)

plt.figure(figsize=(8,5))
plt.plot(
    range(1, len(cumulative_variance) + 1),
    cumulative_variance,
    marker="o"
)
plt.axhline(y=0.90, color="r", linestyle="--")

plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("PCA Scree Plot")
plt.grid(True)
plt.show()

n_components = 10  # adjust if your scree plot suggests a different value

pca_final = PCA(n_components=n_components)
X_pca_final = pca_final.fit_transform(X_scaled)

"""Train/Test Split(PCA DATA)"""

X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(
    X_pca_final,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

pca_model = LogisticRegression(
    max_iter=2000,
    solver="liblinear"
)

pca_model.fit(X_train_pca, y_train_pca)

"""Evaluation:

"""

y_pred_pca = pca_model.predict(X_test_pca)

f1_pca = f1_score(y_test_pca, y_pred_pca)

print("PCA Model F1 Score:", f1_pca)

"""MLflow Logging"""

with mlflow.start_run(run_name="Experiment_5_PCA"):

    mlflow.log_param("pca_components", n_components)
    mlflow.log_param("explained_variance_90pct", cumulative_variance[n_components - 1])

    mlflow.log_metric("test_f1", f1_pca)

    mlflow.sklearn.log_model(pca_model, "model")

"""### üß™ Experiment #5 Summary ‚Äì PCA

In this experiment, Principal Component Analysis (PCA) was applied to reduce
the dimensionality of the feature-engineered dataset. A scree plot was used
to analyze cumulative explained variance and select the number of principal
components that preserve approximately 90% of the variance. A Logistic
Regression model was trained on the reduced feature space and evaluated using
F1-score. All results were logged using MLflow.

## **Experiment $6**
"""

baseline_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", LogisticRegression(
        max_iter=2000,
        solver="liblinear"
    ))
])

baseline_pipeline.fit(X_train, y_train)

y_probs = baseline_pipeline.predict_proba(X_test)[:, 1]

"""Evalute diff threshold"""

from sklearn.metrics import f1_score
import numpy as np

thresholds = np.arange(0.1, 0.9, 0.05)
f1_scores = []

for t in thresholds:
    y_pred_thresh = (y_probs >= t).astype(int)
    f1_scores.append(f1_score(y_test, y_pred_thresh))

best_threshold = thresholds[np.argmax(f1_scores)]
best_f1 = max(f1_scores)

print("Best Threshold:", best_threshold)
print("Best F1 Score:", best_f1)

"""Threshold vs F1 Plot"""

import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
plt.plot(thresholds, f1_scores, marker="o")
plt.axvline(best_threshold, color="r", linestyle="--", label=f"Best Threshold = {best_threshold}")
plt.xlabel("Decision Threshold")
plt.ylabel("F1 Score")
plt.title("Threshold Tuning for F1 Optimization")
plt.legend()
plt.grid(True)
plt.show()

"""final evaluation with best threshold"""

y_pred_best = (y_probs >= best_threshold).astype(int)

final_f1 = f1_score(y_test, y_pred_best)

print("Final Tuned F1 Score:", final_f1)

"""Mlflow logging"""

with mlflow.start_run(run_name="Experiment_6_Threshold_Tuning"):

    mlflow.log_param("custom_experiment", "threshold_tuning")
    mlflow.log_param("best_threshold", best_threshold)

    mlflow.log_metric("best_f1", best_f1)
    mlflow.log_metric("final_f1", final_f1)

    mlflow.sklearn.log_model(baseline_pipeline, "model")

"""### üß™ Experiment #6 Summary ‚Äì Threshold Tuning

In this custom experiment, the default classification threshold of 0.5 was
re-evaluated to improve performance on an imbalanced dataset. Prediction
probabilities from a Logistic Regression model were used to test multiple
decision thresholds. The threshold that maximized F1-score was selected and
used for final evaluation. This experiment demonstrates how decision boundary
adjustments can significantly improve classification performance without
changing the underlying model.

## **Experiment $7**

Compute Class Weights
"""

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

classes = np.unique(y_train)
class_weights = compute_class_weight(
    class_weight="balanced",
    classes=classes,
    y=y_train
)

class_weight_dict = dict(zip(classes, class_weights))
print("Class Weights:", class_weight_dict)

cost_sensitive_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", LogisticRegression(
        max_iter=2000,
        solver="liblinear",
        class_weight=class_weight_dict
    ))
])

cost_sensitive_pipeline.fit(X_train, y_train)

from sklearn.metrics import f1_score, confusion_matrix

y_pred_cost = cost_sensitive_pipeline.predict(X_test)

f1_cost = f1_score(y_test, y_pred_cost)
cm_cost = confusion_matrix(y_test, y_pred_cost)

print("Cost-Sensitive Model F1:", f1_cost)
print("Confusion Matrix:")
print(cm_cost)

with mlflow.start_run(run_name="Experiment_7_Cost_Sensitive_Learning"):

    mlflow.log_param("custom_experiment", "cost_sensitive_learning")
    mlflow.log_param("class_weights", class_weight_dict)

    mlflow.log_metric("test_f1", f1_cost)

    mlflow.sklearn.log_model(cost_sensitive_pipeline, "model")

"""### üß™ Experiment #7 Summary ‚Äì Cost-Sensitive Learning

In this experiment, cost-sensitive learning was applied to address class
imbalance by assigning higher penalties to misclassification of the minority
class. Class weights were computed automatically and used in Logistic
Regression. The model was evaluated using F1-score and compared against
previous experiments. This approach improves minority-class performance
without modifying the dataset distribution.

Save final model Joblib
"""

import joblib
import os

os.makedirs("artifacts", exist_ok=True)

joblib.dump(
    cost_sensitive_pipeline,
    "artifacts/student_risk_model.joblib"
)

print("Final model saved successfully.")

"""‚ÄúBased on all experiments, the cost-sensitive Logistic Regression model
provided the best balance between performance and interpretability, so it
was selected as the final model.‚Äù
"""

import os

os.makedirs("data", exist_ok=True)

df.to_csv("data/final_dataset.csv", index=False)

print("final_dataset.csv saved")

from google.colab import files
files.download("data/final_dataset.csv")

"""Vscode structure

app/

 ‚îú‚îÄ‚îÄ main.py

 ‚îú‚îÄ‚îÄ model.py

 ‚îú‚îÄ‚îÄ schemas.py

 ‚îî‚îÄ‚îÄ requirements.txt

artifacts/

 ‚îî‚îÄ‚îÄ student_risk_model.joblib

"""















"""This is Jus

app/model.py
"""

import os

os.makedirs("data", exist_ok=True)

df.to_csv("data/final_dataset.csv", index=False)

print("final_dataset.csv saved")

from google.colab import files
files.download("data/final_dataset.csv")







# After opening the vs code:(Docker storage)
# (in terminal)

# 1)
# for activating venv
# source venv/bin/activate

# 2)
# python -m uvicorn app.main:app

#next terminal

# 3)
# docker run -p 8000:8000 student-risk-api

# 4)
# http://127.0.0.1:8000/docs

json- replace

{
  "school": "GP",
  "sex": "F",
  "age": 17,
  "address": "U",
  "famsize": "GT3",
  "Pstatus": "T",
  "Medu": 2,
  "Fedu": 2,
  "internet": "yes",
  "studytime": 2,
  "failures": 0,
  "absences": 4,
  "subject": "math"
}